{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic RAG eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU GitPython langchain langchain-openai llama-index ragas ratelimit tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from getpass import getpass\n",
    "\n",
    "from git import Repo\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "from lib.rag.parse_notebooks import read_notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/api-keys\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThrottlingLangchainLLM(LangchainLLMWrapper):\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=3, period=60)\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompts,\n",
    "        n=1,\n",
    "        temperature=1e-8,\n",
    "        callbacks=None,\n",
    "    ):\n",
    "        print(f\"generate_text with {len(prompts)} prompts, n={n}\")\n",
    "        return super().generate(prompts, n, temperature, callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LangchainLLMWrapper.__init__() got an unexpected keyword argument 'llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add custom llms and embeddings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m chatgpt \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n\u001b[0;32m----> 3\u001b[0m generator_llm \u001b[38;5;241m=\u001b[39m \u001b[43mThrottlingLangchainLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchatgpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m critic_llm \u001b[38;5;241m=\u001b[39m ThrottlingLangchainLLM(llm\u001b[38;5;241m=\u001b[39mchatgpt)\n\u001b[1;32m      5\u001b[0m embeddings_model \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n",
      "\u001b[0;31mTypeError\u001b[0m: LangchainLLMWrapper.__init__() got an unexpected keyword argument 'llm'"
     ]
    }
   ],
   "source": [
    "# Add custom llms and embeddings\n",
    "chatgpt = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY)\n",
    "generator_llm = ThrottlingLangchainLLM(llm=chatgpt)\n",
    "critic_llm = ThrottlingLangchainLLM(llm=chatgpt)\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Change resulting question type distribution\n",
    "testset_distribution = {\n",
    "    \"simple\": 0.25,\n",
    "    \"reasoning\": 0.5,\n",
    "    \"multi_context\": 0.0,\n",
    "    \"conditional\": 0.25,\n",
    "}\n",
    "\n",
    "# percentage of conversational question\n",
    "chat_qa = 0.2\n",
    "\n",
    "test_generator = TestsetGenerator(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings_model=embeddings_model,\n",
    "    testset_distribution=testset_distribution,\n",
    "    chat_qa=chat_qa,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_test_dataset(documents, num_test_samples):\n",
    "    return test_generator.generate(documents, num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/elastic/elasticsearch-labs.git\"\n",
    "\n",
    "repo_dir = tempfile.mkdtemp()\n",
    "repo = Repo.clone_from(REPO_URL, repo_dir, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n**Blog: Plagiarism detection with Elasticsearch**\\n\\n```python\\n!pip install elasticsearch==8.11 #Elasticsearch\\n```\\n\\n```python\\npip -q install eland elasticsearch sentence_transformers transformers torch==2.1.0\\n```\\n\\n```python\\nfrom elasticsearch import Elasticsearch, helpers\\nfrom elasticsearch.client import MlClient\\nfrom eland.ml.pytorch import PyTorchModel\\nfrom eland.ml.pytorch.transformers import TransformerModel\\nfrom urllib.request import urlopen\\nimport json\\nfrom pathlib import Path\\nimport getpass\\n```\\n\\n```python\\n# Found in the \\'Manage Deployment\\' page\\nCLOUD_ID = getpass.getpass(\"Enter Elastic Cloud ID:  \")\\n\\n# Password for the \\'elastic\\' user generated by Elasticsearch\\nELASTIC_PASSWORD = getpass.getpass(\"Enter Elastic password:  \")\\n\\n# Create the client instance\\nclient = Elasticsearch(\\n    cloud_id=CLOUD_ID, basic_auth=(\"elastic\", ELASTIC_PASSWORD), request_timeout=3600\\n)\\n```\\n\\n```python\\n# Set the model name from Hugging Face and task type\\n# open ai detector model - developed by open ai https://github.com/openai/gpt-2-output-dataset/tree/master/detector\\nhf_model_id = \"roberta-base-openai-detector\"\\ntm = TransformerModel(model_id=hf_model_id, task_type=\"text_classification\")\\n\\n# set the modelID as it is named in Elasticsearch\\nes_model_id = tm.elasticsearch_model_id()\\n\\n# Download the model from Hugging Face\\ntmp_path = \"models\"\\nPath(tmp_path).mkdir(parents=True, exist_ok=True)\\nmodel_path, config, vocab_path = tm.save(tmp_path)\\n\\n# Load the model into Elasticsearch\\nptm = PyTorchModel(client, es_model_id)\\nptm.import_model(\\n    model_path=model_path, config_path=None, vocab_path=vocab_path, config=config\\n)\\n\\n# Start the model\\ns = MlClient.start_trained_model_deployment(client, model_id=es_model_id)\\ns.body\\n```\\n\\n```python\\n# Set the model name from Hugging Face and task type\\n# sentence-transformers model\\nhf_model_id = \"sentence-transformers/all-mpnet-base-v2\"\\ntm = TransformerModel(model_id=hf_model_id, task_type=\"text_embedding\")\\n\\n# set the modelID as it is named in Elasticsearch\\nes_model_id = tm.elasticsearch_model_id()\\n\\n# Download the model from Hugging Face\\ntmp_path = \"models\"\\nPath(tmp_path).mkdir(parents=True, exist_ok=True)\\nmodel_path, config, vocab_path = tm.save(tmp_path)\\n\\n# Load the model into Elasticsearch\\nptm = PyTorchModel(client, es_model_id)\\nptm.import_model(\\n    model_path=model_path, config_path=None, vocab_path=vocab_path, config=config\\n)\\n\\n# Start the model\\ns = MlClient.start_trained_model_deployment(client, model_id=es_model_id)\\ns.body\\n```\\n\\n```python\\n# source index\\nclient.indices.create(\\n    index=\"plagiarism-docs\",\\n    mappings={\\n        \"properties\": {\\n            \"title\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\\n            \"abstract\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\\n            \"url\": {\"type\": \"keyword\"},\\n            \"venue\": {\"type\": \"keyword\"},\\n            \"year\": {\"type\": \"keyword\"},\\n        }\\n    },\\n)\\n```\\n\\n```python\\n# ingest pipeline\\n\\nclient.ingest.put_pipeline(\\n    id=\"plagiarism-checker-pipeline\",\\n    processors=[\\n        {\\n            \"inference\": {  # for ml models - to infer against the data that is being ingested in the pipeline\\n                \"model_id\": \"roberta-base-openai-detector\",  # text classification model id\\n                \"target_field\": \"openai-detector\",  # Target field for the inference results\\n                \"field_map\": {  # Maps the document field names to the known field names of the model.\\n                    \"abstract\": \"text_field\"  # Field matching our configured trained model input. Typically for NLP models, the field name is text_field.\\n                },\\n            }\\n        },\\n        {\\n            \"inference\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",  # text embedding model model id\\n                \"target_field\": \"abstract_vector\",  # Target field for the inference results\\n                \"field_map\": {  # Maps the document field names to the known field names of the model.\\n                    \"abstract\": \"text_field\"  # Field matching our configured trained model input. Typically for NLP models, the field name is text_field.\\n                },\\n            }\\n        },\\n    ],\\n)\\n```\\n\\n```python\\nclient.indices.create(\\n    index=\"plagiarism-checker\",\\n    mappings={\\n        \"properties\": {\\n            \"title\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\\n            \"abstract\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\\n            \"url\": {\"type\": \"keyword\"},\\n            \"venue\": {\"type\": \"keyword\"},\\n            \"year\": {\"type\": \"keyword\"},\\n            \"abstract_vector.predicted_value\": {  # Inference results field, target_field.predicted_value\\n                \"type\": \"dense_vector\",\\n                \"dims\": 768,  # embedding_size\\n                \"index\": \"true\",\\n                \"similarity\": \"dot_product\",  #  When indexing vectors for approximate kNN search, you need to specify the similarity function for comparing the vectors.\\n            },\\n        }\\n    },\\n)\\n```\\n\\n```python\\nurl = \"https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/emnlp2016-2018.json\"\\n\\n# Send a request to the URL and get the response\\nresponse = urlopen(url)\\n\\n# Load the response data into a JSON object\\ndata_json = json.loads(response.read())\\n\\n\\ndef create_index_body(doc):\\n    \"\"\"Generate the body for an Elasticsearch document.\"\"\"\\n    return {\\n        \"_index\": \"plagiarism-docs\",\\n        \"_source\": doc,\\n    }\\n\\n\\n# Prepare the documents to be indexed\\ndocuments = [create_index_body(doc) for doc in data_json]\\n\\n# Use helpers.bulk to index\\nhelpers.bulk(client, documents)\\n\\nprint(\"Done indexing documents into `plagiarism-docs` source index\")\\n```\\n\\n```python\\n# reindex with ingest pipeline\\n\\nclient.reindex(\\n    wait_for_completion=True,\\n    source={\"index\": \"plagiarism-docs\"},\\n    dest={\"index\": \"plagiarism-checker\", \"pipeline\": \"plagiarism-checker-pipeline\"},\\n)\\n```\\n\\n```python\\n# duplicated text - direct plagiarism\\n\\nmodel_text = \"Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at http://hucvl.github.io/recipeqa.\"\\n\\nresponse = client.search(\\n    index=\"plagiarism-checker\",\\n    size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text,\\n            }\\n        },\\n    },\\n)\\n\\nfor hit in response[\"hits\"][\"hits\"]:\\n    score = hit[\"_score\"]\\n    title = hit[\"_source\"][\"title\"]\\n    abstract = hit[\"_source\"][\"abstract\"]\\n    openai = hit[\"_source\"][\"openai-detector\"][\"predicted_value\"]\\n    url = hit[\"_source\"][\"url\"]\\n\\n    if score > 0.9:\\n        print(f\"\\\\nHigh similarity detected! This might be plagiarism.\")\\n        print(\\n            f\"\\\\nMost similar document: \\'{title}\\'\\\\n\\\\nAbstract: {abstract}\\\\n\\\\nurl: {url}\\\\n\\\\nScore:{score}\\\\n\"\\n        )\\n\\n        if openai == \"Fake\":\\n            print(\"This document may have been created by AI.\\\\n\")\\n\\n    elif score < 0.7:\\n        print(f\"\\\\nLow similarity detected. This might not be plagiarism.\")\\n\\n        if openai == \"Fake\":\\n            print(\"This document may have been created by AI.\\\\n\")\\n\\n    else:\\n        print(f\"\\\\nModerate similarity detected.\")\\n        print(\\n            f\"\\\\nMost similar document: \\'{title}\\'\\\\n\\\\nAbstract: {abstract}\\\\n\\\\nurl: {url}\\\\n\\\\nScore:{score}\\\\n\"\\n        )\\n\\n        if openai == \"Fake\":\\n            print(\"This document may have been created by AI.\\\\n\")\\n\\nml_client = MlClient(client)\\n\\nmodel_id = \"roberta-base-openai-detector\"  # open ai text classification model\\n\\ndocument = [{\"text_field\": model_text}]\\n\\nml_response = ml_client.infer_trained_model(model_id=model_id, docs=document)\\n\\npredicted_value = ml_response[\"inference_results\"][0][\"predicted_value\"]\\n\\nif predicted_value == \"Fake\":\\n    print(\"Note: The text query you entered may have been generated by AI.\\\\n\")\\n```\\n\\n```python\\n# similar text - paraphrase plagiarism\\n\\nmodel_text = \"Comprehending and deducing information from culinary instructions represents a promising avenue for research aimed at empowering artificial intelligence to decipher step-by-step text. In this study, we present CuisineInquiry, a database for the multifaceted understanding of cooking guidelines. It encompasses a substantial number of informative recipes featuring various elements such as headings, explanations, and a matched assortment of visuals. Utilizing an extensive set of automatically crafted question-answer pairings, we formulate a series of tasks focusing on understanding and logic that necessitate a combined interpretation of visuals and written content. This involves capturing the sequential progression of events and extracting meaning from procedural expertise. Our initial findings suggest that CuisineInquiry is poised to function as a demanding experimental platform.\"\\n\\nresponse = client.search(\\n    index=\"plagiarism-checker\",\\n    size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text,\\n            }\\n        },\\n    },\\n)\\n\\nfor hit in response[\"hits\"][\"hits\"]:\\n    score = hit[\"_score\"]\\n    title = hit[\"_source\"][\"title\"]\\n    abstract = hit[\"_source\"][\"abstract\"]\\n    openai = hit[\"_source\"][\"openai-detector\"][\"predicted_value\"]\\n    url = hit[\"_source\"][\"url\"]\\n\\n    if score > 0.9:\\n        print(f\"\\\\nHigh similarity detected! This might be plagiarism.\")\\n        print(\\n            f\"\\\\nMost similar document: \\'{title}\\'\\\\n\\\\nAbstract: {abstract}\\\\n\\\\nurl: {url}\\\\n\\\\nScore:{score}\\\\n\"\\n        )\\n\\n        if openai == \"Fake\":\\n            print(\"This document may have been created by AI.\\\\n\")\\n\\n    elif score < 0.7:\\n        print(f\"\\\\nLow similarity detected. This might not be plagiarism.\")\\n\\n        if openai == \"Fake\":\\n            print(\"This document may have been created by AI.\\\\n\")\\n\\n    else:\\n        print(f\"\\\\nModerate similarity detected.\")\\n        print(\\n            f\"\\\\nMost similar document: \\'{title}\\'\\\\n\\\\nAbstract: {abstract}\\\\n\\\\nurl: {url}\\\\n\\\\nScore:{score}\\\\n\"\\n        )\\n\\n        if openai == \"Fake\":\\n            print(\"This document may have been created by AI.\\\\n\")\\n\\nml_client = MlClient(client)\\n\\nmodel_id = \"roberta-base-openai-detector\"  # open ai text classification model\\n\\ndocument = [{\"text_field\": model_text}]\\n\\nml_response = ml_client.infer_trained_model(model_id=model_id, docs=document)\\n\\npredicted_value = ml_response[\"inference_results\"][0][\"predicted_value\"]\\n\\nif predicted_value == \"Fake\":\\n    print(\"Note: The text query you entered may have been generated by AI.\\\\n\")\\n```\\n', metadata={'repo': 'elasticsearch-labs', 'path': 'supporting-blog-content/plagiarism-detection-with-elasticsearch/plagiarism_detection_es.ipynb', 'title': None}),\n",
       " Document(page_content='\\n<a href=\"https://colab.research.google.com/github/jeffvestal/ElasticDocs_GPT/blob/main/load_embedding_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\\n\\n# ElasticDocs GPT Blog\\n# Loading an embedding from Hugging Face into Elasticsearch\\n\\nThis code will show you how to load a supported embedding model from Hugging Face into an elasticsearch cluster in [Elastic Cloud](https://cloud.elastic.co/)\\n\\n[Blog - ChatGPT and Elasticsearch: OpenAI meets private data](https://www.elastic.co/blog/chatgpt-elasticsearch-openai-meets-private-data)\\n\\n# Setup\\n\\n\\n## Install and import required python libraries\\n\\nElastic uses the [eland python library](https://github.com/elastic/eland) to download modesl from Hugging Face hub and load them into elasticsearch\\n\\n```python\\npip -q install eland elasticsearch sentence_transformers transformers torch==1.11\\n```\\n\\n```python\\nfrom pathlib import Path\\nfrom eland.ml.pytorch import PyTorchModel\\nfrom eland.ml.pytorch.transformers import TransformerModel\\nfrom elasticsearch import Elasticsearch\\nfrom elasticsearch.client import MlClient\\n\\nimport getpass\\n```\\n\\n## Configure elasticsearch authentication. \\nThe recommended authentication approach is using the [Elastic Cloud ID](https://www.elastic.co/guide/en/cloud/current/ec-cloud-id.html) and a [cluster level API key](https://www.elastic.co/guide/en/kibana/current/api-keys.html)\\n\\nYou can use any method you wish to set the required credentials. We are using getpass in this example to prompt for credentials to avoide storing them in github.\\n\\n```python\\nes_cloud_id = getpass.getpass(\"Enter Elastic Cloud ID:  \")\\nes_user = getpass.getpass(\"Enter cluster username:  \")\\nes_pass = getpass.getpass(\"Enter cluster password:  \")\\n\\n# es_api_id = getpass.getpass(\\'Enter cluster API key ID:  \\')\\n# es_api_key = getpass.getpass(\\'Enter cluster API key:  \\')\\n```\\n\\n## Connect to Elastic Cloud\\n\\n```python\\n# es = Elasticsearch(cloud_id=es_cloud_id,\\n#                   api_key=(es_api_id, es_api_key)\\n#                   )\\nes = Elasticsearch(cloud_id=es_cloud_id, basic_auth=(es_user, es_pass))\\nes.info()  # should return cluster info\\n```\\n\\n# Load the model From Hugging Face into Elasticsearch\\nHere we specify the model id from Hugging Face. The easiest way to get this id is clicking the copy the model name icon next to the name on the model page. \\n\\nWhen calling `TransformerModel` you specify the HF model id and the task type. You can try specifying `auto` and eland will attempt to determine the correct type from info in the model config. This is not always possible so a list of specific `task_type` values can be viewed in the following code: \\n[Supported values](https://github.com/elastic/eland/blob/15a300728876022b206161d71055c67b500a0192/eland/ml/pytorch/transformers.py#*L41*)\\n\\n```python\\n# Set the model name from Hugging Face and task type\\nhf_model_id = \"sentence-transformers/all-distilroberta-v1\"\\ntm = TransformerModel(hf_model_id, \"text_embedding\")\\n\\n# set the modelID as it is named in Elasticsearch\\nes_model_id = tm.elasticsearch_model_id()\\n\\n# Download the model from Hugging Face\\ntmp_path = \"models\"\\nPath(tmp_path).mkdir(parents=True, exist_ok=True)\\nmodel_path, config, vocab_path = tm.save(tmp_path)\\n\\n# Load the model into Elasticsearch\\nptm = PyTorchModel(es, es_model_id)\\nptm.import_model(\\n    model_path=model_path, config_path=None, vocab_path=vocab_path, config=config\\n)\\n```\\n\\n# Starting the Model\\n\\n## View information about the model\\nThis is not required but can be handy to get a model overivew\\n\\n```python\\n# List the in elasticsearch\\nm = MlClient.get_trained_models(es, model_id=es_model_id)\\nm.body\\n```\\n\\n## Deploy the model\\nThis will load the model on the ML nodes and start the process(es) making it available for the NLP task\\n\\n```python\\ns = MlClient.start_trained_model_deployment(es, model_id=es_model_id)\\ns.body\\n```\\n\\n## Verify the model started without issue\\nShould output -> {\\'routing_state\\': \\'started\\'}\\n\\n```python\\nstats = MlClient.get_trained_models_stats(es, model_id=es_model_id)\\nstats.body[\"trained_model_stats\"][0][\"deployment_stats\"][\"nodes\"][0][\"routing_state\"]\\n```\\n', metadata={'repo': 'elasticsearch-labs', 'path': 'supporting-blog-content/ElasticDocs_GPT/load_embedding_model.ipynb', 'title': None}),\n",
       " Document(page_content='\\n# Multilingual vector search with E5 embedding models\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/supporting-blog-content/multilingual-e5/multilingual-e5.ipynb)\\n\\nIn this example we\\'ll use a multilingual embedding model\\n[multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) to perform search on a toy dataset of mixed\\nlanguage documents. The examples in this notebook follow the blog post of the same title: Multilingual vector search with E5 embedding models.\\n\\n# 🧰 Requirements\\n\\nFor this example, you will need:\\n\\n- An Elastic Cloud deployment with an ML node (min. 8 GB memory)\\n   - We\\'ll be using [Elastic Cloud](https://www.elastic.co/guide/en/cloud/current/ec-getting-started.html) for this example (available with a [free trial](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook))\\n\\n\\n## Create Elastic Cloud deployment\\n\\nIf you don\\'t have an Elastic Cloud deployment, sign up [here](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook) for a free trial.\\n\\n- Go to the [Create deployment](https://cloud.elastic.co/deployments/create) page\\n   - Select **Create deployment**\\n   - Use the default node types for Elasticsearch and Kibana\\n   - Add an ML node with **8 GB memory** (the multilingual E5 base model is larger than most)\\n\\n# Setup Elasticsearch environment\\n\\nTo get started, we\\'ll need to connect to our Elastic deployment using the Python client.\\nBecause we\\'re using an Elastic Cloud deployment, we\\'ll use the **Cloud ID** to identify our deployment.\\n\\nFirst we need to `pip` install the packages we need for this example.\\n\\n```python\\n!pip install elasticsearch eland[pytorch]\\n```\\n\\nNext we need to import the `elasticsearch` module and the `getpass` module.\\n`getpass` is part of the Python standard library and is used to securely prompt for credentials.\\n\\n```python\\nimport getpass\\n\\nfrom elasticsearch import Elasticsearch\\n```\\n\\nNow we can instantiate the Python Elasticsearch client.\\nFirst we prompt the user for their password and Cloud ID.\\n\\n🔐 NOTE: `getpass` enables us to securely prompt the user for credentials without echoing them to the terminal, or storing it in memory.\\n\\nThen we create a `client` object that instantiates an instance of the `Elasticsearch` class.\\n\\n```python\\n# Found in the \"Manage Deployment\" page\\nCLOUD_ID = getpass.getpass(\"Enter Elastic Cloud ID: \")\\n\\n# Password for the \"elastic\" user generated by Elasticsearch\\nELASTIC_PASSWORD = getpass.getpass(\"Enter Elastic password: \")\\n\\n# Create the client instance\\nclient = Elasticsearch(cloud_id=CLOUD_ID, basic_auth=(\"elastic\", ELASTIC_PASSWORD))\\n\\nclient.info()\\n```\\n\\n# Setup emebdding model\\n\\nNext we upload the E5 multilingual embedding model into Elasticsearch and create an ingest pipeline to automatically create embeddings when ingesting documents. For more details on this process, please see the blog post: [How to deploy NLP: Text Embeddings and Vector Search](https://www.elastic.co/blog/how-to-deploy-nlp-text-embeddings-and-vector-search)\\n\\n```python\\nMODEL_ID = \"multilingual-e5-base\"\\n\\n!eland_import_hub_model \\\\\\n    --cloud-id $CLOUD_ID \\\\\\n    --es-username elastic \\\\\\n    --es-password $ELASTIC_PASSWORD \\\\\\n    --hub-model-id intfloat/$MODEL_ID \\\\\\n    --es-model-id $MODEL_ID \\\\\\n    --task-type text_embedding \\\\\\n    --start\\n```\\n\\n```python\\nclient.ingest.put_pipeline(\\n    id=\"pipeline\",\\n    processors=[\\n        {\\n            \"inference\": {\\n                \"model_id\": MODEL_ID,\\n                \"field_map\": {\"passage\": \"text_field\"},  # field to embed: passage\\n                \"target_field\": \"passage_embedding\",  # embedded field: passage_embedding\\n            }\\n        }\\n    ],\\n)\\n```\\n\\n# Index documents\\n\\nWe need to add a field to support dense vector storage and search.\\nNote the `passage_embedding.predicted_value` field below, which is used to store the dense vector representation of the `passage` field, and will be automatically populated by the inference processor in the pipeline created above. The `passage_embedding` field will also store metadata from the inference process.\\n\\n```python\\n# Define the mapping and settings\\nmapping = {\\n    \"properties\": {\\n        \"id\": {\"type\": \"keyword\"},\\n        \"language\": {\"type\": \"keyword\"},\\n        \"passage\": {\"type\": \"text\"},\\n        \"passage_embedding.predicted_value\": {\\n            \"type\": \"dense_vector\",\\n            \"dims\": 768,\\n            \"index\": \"true\",\\n            \"similarity\": \"cosine\",\\n        },\\n    },\\n    \"_source\": {\"excludes\": [\"passage_embedding.predicted_value\"]},\\n}\\n\\nsettings = {\\n    \"index\": {\\n        \"number_of_replicas\": \"1\",\\n        \"number_of_shards\": \"1\",\\n        \"default_pipeline\": \"pipeline\",\\n    }\\n}\\n\\n# Create the index (deleting any existing index)\\nclient.indices.delete(index=\"passages\", ignore_unavailable=True)\\nclient.indices.create(index=\"passages\", mappings=mapping, settings=settings)\\n```\\n\\nNow that we have the pipeline and mappings ready, we can index our documents. This is of course just a demo so we only index the few toy examples from the blog post.\\n\\n```python\\npassages = [\\n    {\\n        \"id\": \"doc1\",\\n        \"language\": \"en\",\\n        \"passage\": \"\"\"I sat on the bank of the river today.\"\"\",\\n    },\\n    {\\n        \"id\": \"doc2\",\\n        \"language\": \"de\",\\n        \"passage\": \"\"\"Ich bin heute zum Flussufer gegangen.\"\"\",\\n    },\\n    {\\n        \"id\": \"doc3\",\\n        \"language\": \"en\",\\n        \"passage\": \"\"\"I walked to the bank today to deposit money.\"\"\",\\n    },\\n    {\\n        \"id\": \"doc4\",\\n        \"language\": \"de\",\\n        \"passage\": \"\"\"Ich saß heute bei der Bank und wartete auf mein Geld.\"\"\",\\n    },\\n]\\n\\n# Index passages, adding first the \"passage: \" instruction for E5\\nfor doc in passages:\\n    doc[\"passage\"] = f\"\"\"passage: {doc[\"passage\"]}\"\"\"\\n    client.index(index=\"passages\", document=doc)\\n```\\n\\n# Multilingual semantic search\\n\\n```python\\ndef query(q):\\n    \"\"\"Query with embeddings, adding first the \"query: \" instruction for E5.\"\"\"\\n\\n    return client.search(\\n        index=\"passages\",\\n        knn={\\n            \"field\": \"passage_embedding.predicted_value\",\\n            \"query_vector_builder\": {\\n                \"text_embedding\": {\\n                    \"model_id\": MODEL_ID,\\n                    \"model_text\": f\"query: {q}\",\\n                }\\n            },\\n            \"k\": 2,  # for the demo, we\\'re always just searching for pairs of passages\\n            \"num_candidates\": 5,\\n        },\\n    )\\n\\n\\ndef pretty_response(response):\\n    \"\"\"Pretty print search responses.\"\"\"\\n\\n    for hit in response[\"hits\"][\"hits\"]:\\n        score = hit[\"_score\"]\\n        id = hit[\"_source\"][\"id\"]\\n        language = hit[\"_source\"][\"language\"]\\n        passage = hit[\"_source\"][\"passage\"]\\n        print()\\n        print(f\"ID: {id}\")\\n        print(f\"Language: {language}\")\\n        print(f\"Passage: {passage}\")\\n        print(f\"Score: {score}\")\\n```\\n\\n```python\\n# Example 1\\npretty_response(query(\"riverside\"))\\n```\\n\\n```python\\n# Example 2\\npretty_response(query(\"Geldautomat\"))\\n```\\n\\n```python\\n# Example 3a\\npretty_response(query(\"movement\"))\\n```\\n\\n```python\\n# Example 3b\\npretty_response(query(\"stillness\"))\\n```\\n', metadata={'repo': 'elasticsearch-labs', 'path': 'supporting-blog-content/multilingual-e5/multilingual-e5.ipynb', 'title': 'Multilingual vector search with E5 embedding models'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebooks = read_notebooks(REPO_URL, repo_dir)\n",
    "notebooks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A2024-02-07 11:31:00,530 - WARNING - generate 1, 1\n",
      "2024-02-07 11:31:01,414 - WARNING - generate 1, 1\n",
      "2024-02-07 11:31:02,867 - WARNING - generate 1, 1\n",
      "2024-02-07 11:32:00,530 - WARNING - generate 1, 1\n",
      "2024-02-07 11:32:02,880 - WARNING - generate 1, 1\n",
      "2024-02-07 11:32:04,625 - WARNING - generate 1, 1\n",
      "2024-02-07 11:33:00,531 - WARNING - generate 1, 1\n",
      "2024-02-07 11:33:04,341 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:33:07,912 - WARNING - generate 1, 1\n",
      "2024-02-07 11:34:00,535 - WARNING - generate 1, 1\n",
      "2024-02-07 11:34:01,962 - WARNING - generate 1, 1\n",
      "2024-02-07 11:34:03,678 - WARNING - generate 1, 1\n",
      "2024-02-07 11:35:00,536 - WARNING - generate 1, 1\n",
      "2024-02-07 11:35:02,079 - WARNING - generate 1, 1\n",
      "2024-02-07 11:35:03,768 - WARNING - generate 1, 1\n",
      "2024-02-07 11:36:00,537 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:36:03,102 - WARNING - generate 1, 1\n",
      "2024-02-07 11:36:03,686 - WARNING - generate 1, 1\n",
      "2024-02-07 11:37:00,541 - WARNING - generate 1, 1\n",
      "2024-02-07 11:37:02,237 - WARNING - generate 1, 1\n",
      "2024-02-07 11:37:02,964 - WARNING - generate 1, 1\n",
      "2024-02-07 11:38:00,546 - WARNING - generate 1, 1\n",
      "2024-02-07 11:38:01,350 - WARNING - generate 1, 1\n",
      "2024-02-07 11:38:02,697 - WARNING - generate 1, 1\n",
      "2024-02-07 11:39:00,549 - WARNING - generate 1, 1\n",
      "2024-02-07 11:39:03,019 - WARNING - generate 1, 1\n",
      "2024-02-07 11:39:05,111 - WARNING - generate 1, 1\n",
      "2024-02-07 11:40:00,550 - WARNING - generate 1, 1\n",
      "2024-02-07 11:40:06,993 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:40:10,408 - WARNING - generate 1, 1\n",
      "2024-02-07 11:41:00,549 - WARNING - generate 1, 1\n",
      "2024-02-07 11:41:01,823 - WARNING - generate 1, 1\n",
      "2024-02-07 11:41:03,657 - WARNING - generate 1, 1\n",
      "2024-02-07 11:42:00,552 - WARNING - generate 1, 1\n",
      "2024-02-07 11:42:18,567 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:42:20,497 - WARNING - generate 1, 1\n",
      "2024-02-07 11:43:00,555 - WARNING - generate 1, 1\n",
      "2024-02-07 11:43:01,413 - WARNING - generate 1, 1\n",
      "2024-02-07 11:43:02,831 - WARNING - generate 1, 1\n",
      "2024-02-07 11:44:00,560 - WARNING - generate 1, 1\n",
      "2024-02-07 11:44:03,858 - WARNING - generate 1, 1\n",
      "2024-02-07 11:44:05,959 - WARNING - generate 1, 1\n",
      "2024-02-07 11:45:00,564 - WARNING - generate 1, 1\n",
      "2024-02-07 11:45:13,183 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:45:15,799 - WARNING - generate 1, 1\n",
      "2024-02-07 11:46:00,568 - WARNING - generate 1, 1\n",
      "2024-02-07 11:46:01,319 - WARNING - generate 1, 1\n",
      "2024-02-07 11:46:02,531 - WARNING - generate 1, 1\n",
      "2024-02-07 11:47:00,570 - WARNING - generate 1, 1\n",
      "/Users/maxjakob/.pyenv/versions/3.11.7/lib/python3.11/site-packages/ragas/testset/testset_generator.py:279: UserWarning: No neighbors exists\n",
      "  warnings.warn(\"No neighbors exists\")\n",
      "2024-02-07 11:47:01,249 - WARNING - generate 1, 1\n",
      "2024-02-07 11:47:01,823 - WARNING - generate 1, 1\n",
      "2024-02-07 11:48:00,573 - WARNING - generate 1, 1\n",
      "2024-02-07 11:48:03,186 - WARNING - generate 1, 1\n",
      "2024-02-07 11:48:05,834 - WARNING - generate 1, 1\n",
      "2024-02-07 11:49:00,573 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:49:03,533 - WARNING - generate 1, 1\n",
      "2024-02-07 11:49:04,378 - WARNING - generate 1, 1\n",
      "2024-02-07 11:50:00,577 - WARNING - generate 1, 1\n",
      "2024-02-07 11:50:02,376 - WARNING - generate 1, 1\n",
      "2024-02-07 11:50:05,345 - WARNING - generate 1, 1\n",
      "2024-02-07 11:51:00,581 - WARNING - generate 1, 1\n",
      "2024-02-07 11:51:03,025 - WARNING - generate 1, 1\n",
      "2024-02-07 11:51:12,766 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:52:00,585 - WARNING - generate 1, 1\n",
      "2024-02-07 11:52:01,256 - WARNING - generate 1, 1\n",
      "2024-02-07 11:52:01,825 - WARNING - generate 1, 1\n",
      "2024-02-07 11:53:00,589 - WARNING - generate 1, 1\n",
      "2024-02-07 11:53:01,917 - WARNING - generate 1, 1\n",
      "2024-02-07 11:53:03,409 - WARNING - generate 1, 1\n",
      "2024-02-07 11:54:00,590 - WARNING - generate 1, 1\n",
      "2024-02-07 11:54:02,415 - WARNING - generate 1, 1\n",
      "2024-02-07 11:54:04,535 - WARNING - generate 1, 1\n",
      "2024-02-07 11:55:00,590 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:55:05,689 - WARNING - generate 1, 1\n",
      "2024-02-07 11:55:06,392 - WARNING - generate 1, 1\n",
      "2024-02-07 11:56:00,589 - WARNING - generate 1, 1\n",
      "2024-02-07 11:56:02,684 - WARNING - generate 1, 1\n",
      "2024-02-07 11:56:06,241 - WARNING - generate 1, 1\n",
      "2024-02-07 11:57:00,594 - WARNING - generate 1, 1\n",
      "2024-02-07 11:57:02,615 - WARNING - generate 1, 1\n",
      "2024-02-07 11:57:42,159 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 11:58:00,595 - WARNING - generate 1, 1\n",
      "2024-02-07 11:58:01,309 - WARNING - generate 1, 1\n",
      "2024-02-07 11:58:02,169 - WARNING - generate 1, 1\n",
      "2024-02-07 11:59:00,595 - WARNING - generate 1, 1\n",
      "2024-02-07 11:59:02,232 - WARNING - generate 1, 1\n",
      "2024-02-07 11:59:03,768 - WARNING - generate 1, 1\n",
      "2024-02-07 12:00:00,596 - WARNING - generate 1, 1\n",
      "2024-02-07 12:00:03,268 - WARNING - generate 1, 1\n",
      "2024-02-07 12:00:04,731 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 12:01:00,600 - WARNING - generate 1, 1\n",
      "2024-02-07 12:01:01,425 - WARNING - generate 1, 1\n",
      "2024-02-07 12:01:02,607 - WARNING - generate 1, 1\n",
      "2024-02-07 12:02:00,604 - WARNING - generate 1, 1\n",
      "2024-02-07 12:02:02,396 - WARNING - generate 1, 1\n",
      "2024-02-07 12:02:04,440 - WARNING - generate 1, 1\n",
      "2024-02-07 12:03:00,605 - WARNING - generate 1, 1\n",
      "2024-02-07 12:03:03,502 - WARNING - generate 1, 1\n",
      "2024-02-07 12:03:06,423 - WARNING - generate 1, 1\n",
      "2024-02-07 12:04:00,609 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 12:04:03,750 - WARNING - generate 1, 1\n",
      "2024-02-07 12:04:04,367 - WARNING - generate 1, 1\n",
      "2024-02-07 12:05:00,613 - WARNING - generate 1, 1\n",
      "2024-02-07 12:05:02,431 - WARNING - generate 1, 1\n",
      "2024-02-07 12:05:04,286 - WARNING - generate 1, 1\n",
      "2024-02-07 12:06:00,615 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A2024-02-07 12:06:02,352 - WARNING - generate 1, 1\n",
      "2024-02-07 12:06:03,360 - WARNING - generate 1, 1\n",
      "2024-02-07 12:07:00,614 - WARNING - generate 1, 1\n",
      "2024-02-07 12:07:02,841 - WARNING - generate 1, 1\n",
      "2024-02-07 12:07:05,349 - WARNING - generate 1, 1\n",
      "2024-02-07 12:08:00,618 - WARNING - generate 1, 1\n",
      "2024-02-07 12:08:03,363 - WARNING - generate 1, 1\n",
      "2024-02-07 12:08:06,566 - WARNING - generate 1, 1\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestDataset(test_data=[DataRow(question='What does the `reindex` function in the code serve? If successful, what fields will the `dest` index have?', ground_truth_context=['Now we can reindex data from the `source` index `ecommerce` to the `dest` index `ecommerce-search` with the ingest pipeline `ecommerce-pipeline` we created.\\nAfter this step our `dest` index will have the fields we need to perform Semantic Search.'], ground_truth=['The `reindex` function in the code serves to transfer data from the `source` index `ecommerce` to the `dest` index `ecommerce-search` using the ingest pipeline `ecommerce-pipeline`. If successful, the `dest` index will have the fields required to perform Semantic Search.'], question_type='conditional', episode_done=True), DataRow(question='What are the model ID and task type for loading the Hugging Face model into Elasticsearch?', ground_truth_context=['When calling `TransformerModel` you specify the HF model id and the task type.\\nYou can try specifying `auto` and eland will attempt to determine the correct type from info in the model config.\\nThis is not always possible so a list of specific `task_type` values can be viewed in the following code: \\n[Supported values](https://github.com/elastic/eland/blob/15a300728876022b206161d71055c67b500a0192/eland/ml/pytorch/transformers.py#*L41*)\\nhf_model_id=\\'sentence-transformers/all-distilroberta-v1\\'\\ntm = TransformerModel(hf_model_id, \"text_embedding\")'], ground_truth=['The model ID for loading the Hugging Face model into Elasticsearch is \\'sentence-transformers/all-distilroberta-v1\\' and the task type is \"text_embedding\".'], question_type='reasoning', episode_done=True), DataRow(question='What is the purpose of the `dataframe_to_bulk_actions` function in the Elasticsearch indexing process for the `wikipedia_vector_index`?', ground_truth_context=[\"The following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently in a single request.\\nFor each row in the DataFrame, the function yields a dictionary representing a single document to be indexed.\\nThe dataframe is large, we will index data in batches of `100`. We index the data into Elasticsearch using the Python client's [helpers](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/client-helpers.html#bulk-helpers) for the bulk API.\"], ground_truth=[\"The purpose of the `dataframe_to_bulk_actions` function in the Elasticsearch indexing process for the `wikipedia_vector_index` is to generate the required bulk actions that can be passed to Elasticsearch's Bulk API. This allows for efficient indexing of multiple documents in a single request.\"], question_type='reasoning', episode_done=True), DataRow(question='What is the purpose of the Elasticsearch document with the field name \"abstract\"?', ground_truth_context=['- \"abstract\": \"text_field\" # Field matching our configured trained model input. Typically for NLP models, the field name is text_field.\\n- \"abstract\": {\\n        \"type\": \"text\",\\n        \"fields\": {\\n            \"keyword\": {\\n                \"type\": \"keyword\"\\n            }\\n        }\\n    }\\n- \"abstract_vector.predicted_value\": { # Inference results field, target_field.predicted_value\\n    \"type\": \"dense_vector\",\\n    \"dims\": 768, # embedding_size\\n    \"index\": \"true\",\\n    \"similarity\": \"dot_product\" #  When indexing vectors for approximate kNN search, you need to specify the similarity function for comparing the vectors.\\n         }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- print(f\"\\\\nMost similar document: \\'{title}\\'\\\\n\\\\nAbstract: {abstract}\\\\n\\\\nurl: {url}\\\\n\\\\nScore:{score}\\\\n\")\\n- model_id = \\'roberta-base-openai-detector\\' #open ai text classification model\\n- document = [\\n    {\\n        \"text_field\": model_text\\n    }\\n]'], ground_truth=['The purpose of the Elasticsearch document with the field name \"abstract\" is to store the abstract text of a document.'], question_type='simple', episode_done=True), DataRow(question='What is the purpose of the `eland_import_hub_model` command in relation to the Elasticsearch index for the passages?', ground_truth_context=['- Next we upload the E5 multilingual embedding model into Elasticsearch and create an ingest pipeline to automatically create embeddings when ingesting documents.\\n- For more details on this process, please see the blog post: [How to deploy NLP: Text Embeddings and Vector Search](https://www.elastic.co/blog/how-to-deploy-nlp-text-embeddings-and-vector-search)\\n- Now we can instantiate the Python Elasticsearch client.\\n- Next we need to import the `elasticsearch` module and the `getpass` module.\\n- Then we create a `client` object that instantiates an instance of the `Elasticsearch` class.\\n- Next we need to connect to our Elastic deployment using the Python client.\\n- First we prompt the user for their password and Cloud ID.\\n- Then we create a `client` object that instantiates an instance of the `Elasticsearch` class.\\n- Next we upload the E5 multilingual embedding model into Elasticsearch and create an ingest pipeline to automatically create embeddings when ingesting documents.\\n- Now that we have the pipeline and mappings ready, we can index our documents.\\n- Now we can index our documents.'], ground_truth=['The purpose of the `eland_import_hub_model` command is to upload the E5 multilingual embedding model into Elasticsearch and create an ingest pipeline to automatically create embeddings when ingesting documents.'], question_type='reasoning', episode_done=True), DataRow(question='What are the top 5 winning streaks of the Boston Celtics in the current season?', ground_truth_context=[\"- The Celtics won {games_won['count']} games this season so far.\\n- top_streaks = sorted(streaks, key=lambda x: x[1], reverse=True)[:5]\"], ground_truth=['The information provided does not include the specific number of games won by the Boston Celtics in the current season or the top 5 winning streaks. Therefore, it is not possible to answer the question using the given context.'], question_type='simple', episode_done=True), DataRow(question='What does the code `os.environ[\\'es_cloud_id\\'] = getpass(\"Elastic deployment Cloud ID\")` do in the context of building a Generative AI app with Elasticsearch and OpenAI?', ground_truth_context=['- To connect to Elasticsearch, you need to create a client instance with the Cloud ID and password for your deployment.\\n- `os.environ[\\'es_cloud_id\\'] = getpass(\"Elastic deployment Cloud ID\")` sets the Cloud ID for the Elasticsearch deployment.\\n- `os.environ[\\'es_password\\'] = getpass(\"Elastic deployment Password\")` sets the password for the Elasticsearch deployment.\\n- `client = Elasticsearch(cloud_id = es_cloud_id, basic_auth=(\"elastic\", es_password))` creates a client instance to connect to Elasticsearch using the Cloud ID and password.\\n- `print(client.info())` tests the connection to Elasticsearch.\\n- `es_cloud_id = os.environ[\\'es_cloud_id\\']` retrieves the Cloud ID from the environment variables.\\n- `es_password = os.environ[\\'es_password\\']` retrieves the password from the environment variables.'], ground_truth=['The code `os.environ[\\'es_cloud_id\\'] = getpass(\"Elastic deployment Cloud ID\")` sets the Cloud ID for the Elasticsearch deployment.'], question_type='reasoning', episode_done=True), DataRow(question='What is the purpose and usage of the `dataframe_to_bulk_actions` function in relation to indexing data into Elasticsearch?', ground_truth_context=[\"The following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently in a single request.\\nFor each row in the DataFrame, the function yields a dictionary representing a single document to be indexed.\\nAs the dataframe is large, we will index data in batches of `100`. We index the data into Elasticsearch using the Python client's [helpers](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/client-helpers.html#bulk-helpers) for the bulk API.\"], ground_truth=[\"The purpose of the `dataframe_to_bulk_actions` function is to generate the necessary bulk actions that can be used to index multiple documents efficiently in a single request to Elasticsearch's Bulk API. The function takes a DataFrame as input and for each row in the DataFrame, it yields a dictionary representing a single document to be indexed. The data is indexed in batches of 100 using the Python client's helpers for the bulk API.\"], question_type='reasoning', episode_done=True), DataRow(question='What is the purpose of the Elasticsearch document with the field name \"abstract\" and its relation to the \"plagiarism-checker\" index and \"plagiarism-checker-pipeline\" pipeline?', ground_truth_context=['- \"abstract\": \"text_field\" # Field matching our configured trained model input. Typically for NLP models, the field name is text_field.\\n- \"abstract\": {\\n        \"type\": \"text\",\\n        \"fields\": {\\n            \"keyword\": {\\n                \"type\": \"keyword\"\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }'], ground_truth=['The purpose of the Elasticsearch document with the field name \"abstract\" is to store the text content that will be used as input for the trained model. It is related to the \"plagiarism-checker\" index and \"plagiarism-checker-pipeline\" pipeline as it is used in the search queries to find similar documents based on the vector representation of the \"abstract\" field.'], question_type='reasoning', episode_done=True), DataRow(question='What is the recommended authentication approach for elasticsearch in Elastic Cloud with a cluster level API key?', ground_truth_context=['The recommended authentication approach is using the Elastic Cloud ID and a cluster level API key.'], ground_truth=['The recommended authentication approach for elasticsearch in Elastic Cloud with a cluster level API key is using the Elastic Cloud ID and a cluster level API key.'], question_type='conditional', episode_done=True), DataRow(question='How can the ingest pipeline in Elasticsearch be utilized for text search?', ground_truth_context=['Next we upload the all-mpnet-base-v2 embedding model into Elasticsearch and create an ingest pipeline with inference processors for text embedding and text expansion, using the description field for both. This field contains the description of each product.'], ground_truth=['The ingest pipeline in Elasticsearch can be utilized for text search by uploading an embedding model and creating an ingest pipeline with inference processors. The processors can be used for text embedding and text expansion, using the description field of each product. This allows for more efficient and accurate text search capabilities in Elasticsearch.'], question_type='reasoning', episode_done=False), DataRow(question='In what ways can the ingest pipeline in Elasticsearch be used for lexical and semantic search?', ground_truth_context=['Next we upload the all-mpnet-base-v2 embedding model into Elasticsearch and create an ingest pipeline with inference processors for text embedding and text expansion, using the description field for both. This field contains the description of each product.'], ground_truth=['The ingest pipeline in Elasticsearch can be used for lexical and semantic search by utilizing the text embedding and text expansion inference processors. These processors analyze the description field of each product and generate embeddings or expand the text to enhance the search capabilities.'], question_type='reasoning', episode_done=True), DataRow(question='What is the target field for the inference results in the `ecommerce-search` index?', ground_truth_context=['For the `ecommerce-search` index we add a field to support dense vector storage and search `description_vector.predicted_value`, this is the target field for inference results.'], ground_truth=['The target field for the inference results in the `ecommerce-search` index is `description_vector.predicted_value`.'], question_type='simple', episode_done=True), DataRow(question='Which passages are indexed in the \"passages\" index for the documents with IDs \"doc1\", \"doc2\", \"doc3\", and \"doc4\"?', ground_truth_context=['- Create the index (deleting any existing index)\\n- Index passages, adding first the \"passage: \" instruction for E5\\n- Query with embeddings, adding first the \"query: \" instruction for E5\\n- Pretty print search responses'], ground_truth=['The given context does not provide any information about the passages indexed for the documents with IDs \"doc1\", \"doc2\", \"doc3\", and \"doc4\".'], question_type='reasoning', episode_done=True)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_testset = generate_test_dataset(notebooks[:10], 20)\n",
    "nb_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does the `reindex` function in the code serve? If successful, what fields will the `dest` index have?\n",
      "['The `reindex` function in the code serves to transfer data from the `source` index `ecommerce` to the `dest` index `ecommerce-search` using the ingest pipeline `ecommerce-pipeline`. If successful, the `dest` index will have the fields required to perform Semantic Search.']\n",
      "['Now we can reindex data from the `source` index `ecommerce` to the `dest` index `ecommerce-search` with the ingest pipeline `ecommerce-pipeline` we created.\\nAfter this step our `dest` index will have the fields we need to perform Semantic Search.']\n",
      "\n",
      "What are the model ID and task type for loading the Hugging Face model into Elasticsearch?\n",
      "['The model ID for loading the Hugging Face model into Elasticsearch is \\'sentence-transformers/all-distilroberta-v1\\' and the task type is \"text_embedding\".']\n",
      "['When calling `TransformerModel` you specify the HF model id and the task type.\\nYou can try specifying `auto` and eland will attempt to determine the correct type from info in the model config.\\nThis is not always possible so a list of specific `task_type` values can be viewed in the following code: \\n[Supported values](https://github.com/elastic/eland/blob/15a300728876022b206161d71055c67b500a0192/eland/ml/pytorch/transformers.py#*L41*)\\nhf_model_id=\\'sentence-transformers/all-distilroberta-v1\\'\\ntm = TransformerModel(hf_model_id, \"text_embedding\")']\n",
      "\n",
      "What is the purpose of the `dataframe_to_bulk_actions` function in the Elasticsearch indexing process for the `wikipedia_vector_index`?\n",
      "[\"The purpose of the `dataframe_to_bulk_actions` function in the Elasticsearch indexing process for the `wikipedia_vector_index` is to generate the required bulk actions that can be passed to Elasticsearch's Bulk API. This allows for efficient indexing of multiple documents in a single request.\"]\n",
      "[\"The following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently in a single request.\\nFor each row in the DataFrame, the function yields a dictionary representing a single document to be indexed.\\nThe dataframe is large, we will index data in batches of `100`. We index the data into Elasticsearch using the Python client's [helpers](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/client-helpers.html#bulk-helpers) for the bulk API.\"]\n",
      "\n",
      "What is the purpose of the Elasticsearch document with the field name \"abstract\"?\n",
      "['The purpose of the Elasticsearch document with the field name \"abstract\" is to store the abstract text of a document.']\n",
      "['- \"abstract\": \"text_field\" # Field matching our configured trained model input. Typically for NLP models, the field name is text_field.\\n- \"abstract\": {\\n        \"type\": \"text\",\\n        \"fields\": {\\n            \"keyword\": {\\n                \"type\": \"keyword\"\\n            }\\n        }\\n    }\\n- \"abstract_vector.predicted_value\": { # Inference results field, target_field.predicted_value\\n    \"type\": \"dense_vector\",\\n    \"dims\": 768, # embedding_size\\n    \"index\": \"true\",\\n    \"similarity\": \"dot_product\" #  When indexing vectors for approximate kNN search, you need to specify the similarity function for comparing the vectors.\\n         }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- print(f\"\\\\nMost similar document: \\'{title}\\'\\\\n\\\\nAbstract: {abstract}\\\\n\\\\nurl: {url}\\\\n\\\\nScore:{score}\\\\n\")\\n- model_id = \\'roberta-base-openai-detector\\' #open ai text classification model\\n- document = [\\n    {\\n        \"text_field\": model_text\\n    }\\n]']\n",
      "\n",
      "What is the purpose of the `eland_import_hub_model` command in relation to the Elasticsearch index for the passages?\n",
      "['The purpose of the `eland_import_hub_model` command is to upload the E5 multilingual embedding model into Elasticsearch and create an ingest pipeline to automatically create embeddings when ingesting documents.']\n",
      "['- Next we upload the E5 multilingual embedding model into Elasticsearch and create an ingest pipeline to automatically create embeddings when ingesting documents.\\n- For more details on this process, please see the blog post: [How to deploy NLP: Text Embeddings and Vector Search](https://www.elastic.co/blog/how-to-deploy-nlp-text-embeddings-and-vector-search)\\n- Now we can instantiate the Python Elasticsearch client.\\n- Next we need to import the `elasticsearch` module and the `getpass` module.\\n- Then we create a `client` object that instantiates an instance of the `Elasticsearch` class.\\n- Next we need to connect to our Elastic deployment using the Python client.\\n- First we prompt the user for their password and Cloud ID.\\n- Then we create a `client` object that instantiates an instance of the `Elasticsearch` class.\\n- Next we upload the E5 multilingual embedding model into Elasticsearch and create an ingest pipeline to automatically create embeddings when ingesting documents.\\n- Now that we have the pipeline and mappings ready, we can index our documents.\\n- Now we can index our documents.']\n",
      "\n",
      "What are the top 5 winning streaks of the Boston Celtics in the current season?\n",
      "['The information provided does not include the specific number of games won by the Boston Celtics in the current season or the top 5 winning streaks. Therefore, it is not possible to answer the question using the given context.']\n",
      "[\"- The Celtics won {games_won['count']} games this season so far.\\n- top_streaks = sorted(streaks, key=lambda x: x[1], reverse=True)[:5]\"]\n",
      "\n",
      "What does the code `os.environ['es_cloud_id'] = getpass(\"Elastic deployment Cloud ID\")` do in the context of building a Generative AI app with Elasticsearch and OpenAI?\n",
      "['The code `os.environ[\\'es_cloud_id\\'] = getpass(\"Elastic deployment Cloud ID\")` sets the Cloud ID for the Elasticsearch deployment.']\n",
      "['- To connect to Elasticsearch, you need to create a client instance with the Cloud ID and password for your deployment.\\n- `os.environ[\\'es_cloud_id\\'] = getpass(\"Elastic deployment Cloud ID\")` sets the Cloud ID for the Elasticsearch deployment.\\n- `os.environ[\\'es_password\\'] = getpass(\"Elastic deployment Password\")` sets the password for the Elasticsearch deployment.\\n- `client = Elasticsearch(cloud_id = es_cloud_id, basic_auth=(\"elastic\", es_password))` creates a client instance to connect to Elasticsearch using the Cloud ID and password.\\n- `print(client.info())` tests the connection to Elasticsearch.\\n- `es_cloud_id = os.environ[\\'es_cloud_id\\']` retrieves the Cloud ID from the environment variables.\\n- `es_password = os.environ[\\'es_password\\']` retrieves the password from the environment variables.']\n",
      "\n",
      "What is the purpose and usage of the `dataframe_to_bulk_actions` function in relation to indexing data into Elasticsearch?\n",
      "[\"The purpose of the `dataframe_to_bulk_actions` function is to generate the necessary bulk actions that can be used to index multiple documents efficiently in a single request to Elasticsearch's Bulk API. The function takes a DataFrame as input and for each row in the DataFrame, it yields a dictionary representing a single document to be indexed. The data is indexed in batches of 100 using the Python client's helpers for the bulk API.\"]\n",
      "[\"The following function generates the required bulk actions that can be passed to Elasticsearch's Bulk API, so we can index multiple documents efficiently in a single request.\\nFor each row in the DataFrame, the function yields a dictionary representing a single document to be indexed.\\nAs the dataframe is large, we will index data in batches of `100`. We index the data into Elasticsearch using the Python client's [helpers](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/client-helpers.html#bulk-helpers) for the bulk API.\"]\n",
      "\n",
      "What is the purpose of the Elasticsearch document with the field name \"abstract\" and its relation to the \"plagiarism-checker\" index and \"plagiarism-checker-pipeline\" pipeline?\n",
      "['The purpose of the Elasticsearch document with the field name \"abstract\" is to store the text content that will be used as input for the trained model. It is related to the \"plagiarism-checker\" index and \"plagiarism-checker-pipeline\" pipeline as it is used in the search queries to find similar documents based on the vector representation of the \"abstract\" field.']\n",
      "['- \"abstract\": \"text_field\" # Field matching our configured trained model input. Typically for NLP models, the field name is text_field.\\n- \"abstract\": {\\n        \"type\": \"text\",\\n        \"fields\": {\\n            \"keyword\": {\\n                \"type\": \"keyword\"\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }\\n- response = client.search(index=\\'plagiarism-checker\\', size=1,\\n    knn={\\n        \"field\": \"abstract_vector.predicted_value\",\\n        \"k\": 9,\\n        \"num_candidates\": 974,\\n        \"query_vector_builder\": {\\n            \"text_embedding\": {\\n                \"model_id\": \"sentence-transformers__all-mpnet-base-v2\",\\n                \"model_text\": model_text\\n            }\\n        }\\n    }']\n",
      "\n",
      "What is the recommended authentication approach for elasticsearch in Elastic Cloud with a cluster level API key?\n",
      "['The recommended authentication approach for elasticsearch in Elastic Cloud with a cluster level API key is using the Elastic Cloud ID and a cluster level API key.']\n",
      "['The recommended authentication approach is using the Elastic Cloud ID and a cluster level API key.']\n",
      "\n",
      "How can the ingest pipeline in Elasticsearch be utilized for text search?\n",
      "['The ingest pipeline in Elasticsearch can be utilized for text search by uploading an embedding model and creating an ingest pipeline with inference processors. The processors can be used for text embedding and text expansion, using the description field of each product. This allows for more efficient and accurate text search capabilities in Elasticsearch.']\n",
      "['Next we upload the all-mpnet-base-v2 embedding model into Elasticsearch and create an ingest pipeline with inference processors for text embedding and text expansion, using the description field for both. This field contains the description of each product.']\n",
      "\n",
      "In what ways can the ingest pipeline in Elasticsearch be used for lexical and semantic search?\n",
      "['The ingest pipeline in Elasticsearch can be used for lexical and semantic search by utilizing the text embedding and text expansion inference processors. These processors analyze the description field of each product and generate embeddings or expand the text to enhance the search capabilities.']\n",
      "['Next we upload the all-mpnet-base-v2 embedding model into Elasticsearch and create an ingest pipeline with inference processors for text embedding and text expansion, using the description field for both. This field contains the description of each product.']\n",
      "\n",
      "What is the target field for the inference results in the `ecommerce-search` index?\n",
      "['The target field for the inference results in the `ecommerce-search` index is `description_vector.predicted_value`.']\n",
      "['For the `ecommerce-search` index we add a field to support dense vector storage and search `description_vector.predicted_value`, this is the target field for inference results.']\n",
      "\n",
      "Which passages are indexed in the \"passages\" index for the documents with IDs \"doc1\", \"doc2\", \"doc3\", and \"doc4\"?\n",
      "['The given context does not provide any information about the passages indexed for the documents with IDs \"doc1\", \"doc2\", \"doc3\", and \"doc4\".']\n",
      "['- Create the index (deleting any existing index)\\n- Index passages, adding first the \"passage: \" instruction for E5\\n- Query with embeddings, adding first the \"query: \" instruction for E5\\n- Pretty print search responses']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in nb_testset.test_data:\n",
    "    print(data.question)\n",
    "    print(data.ground_truth)\n",
    "    print(data.ground_truth_context)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
